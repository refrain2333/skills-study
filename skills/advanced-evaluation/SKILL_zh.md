---
name: 高级评估
description: 掌握LLM作为判官的评估技术，包括直接评分、成对比较、评分标准生成和偏差缓解。在构建评估系统、比较模型输出或为AI生成的内容建立质量标准时使用。
---

# 高级评估

本技能涵盖使用LLM作为判官评估LLM输出的生产级技术。它将学术论文、行业实践和实际实现经验的研究综合成可操作的模式，用于构建可靠的评估系统。

**关键洞察**：LLM作为判官不是单一的技术，而是一族方法，每种都适合不同的评估环境。选择正确的方法和缓解已知的偏差是本技能的核心能力。

## 何时激活

激活本技能的场景：

- 为LLM输出构建自动化评估流程
- 比较多个模型响应以选择最佳响应
- 在评估团队间建立一致的质量标准
- 调试显示不一致结果的评估系统
- 设计提示词或模型变化的A/B测试
- 为人工或自动化评估创建评分标准
- 分析自动化与人工判断之间的相关性

## 核心概念

### 评估分类学

评估方法分为两个主要类别，具有不同的可靠性特征：

**直接评分**：单个LLM在定义的量表上评价一个响应。
- 最佳用途：客观标准（事实准确性、指令遵循、有害内容检测）
- 可靠性：定义清晰的标准下中等到高
- 失败模式：评分校准漂移、不一致的量表解释

**成对比较**：LLM比较两个响应并选择更好的那个。
- 最佳用途：主观偏好（语气、风格、说服力）
- 可靠性：高于直接评分（用于偏好）
- 失败模式：位置偏差、长度偏差

来自MT-Bench论文（Zheng等，2023）的研究表明，成对比较与直接评分相比能达到与人类判官更高的一致性，用于基于偏好的评估，而直接评分仍然适合具有明确基准真值的客观标准。

### 偏差景观

LLM判官表现出必须主动缓解的系统性偏差：

**位置偏差**：成对比较中的第一位置响应获得优先处理。缓解方法：评估两次并交换位置，使用多数投票或一致性检查。

**长度偏差**：较长的响应被评分更高，无论质量如何。缓解方法：明确提示忽略长度、长度归一化评分。

**自我提升偏差**：模型对自己的输出评分更高。缓解方法：使用不同的模型进行生成和评估，或承认限制。

**冗长偏差**：详细的解释获得更高的评分，即使不必要。缓解方法：标准特定的评分标准，对不相关的细节进行惩罚。

**权威偏差**：自信、权威的语气无论准确性如何都被评分更高。缓解方法：要求证据引用、事实检查层。

### 指标选择框架

根据评估任务的结构选择指标：

| 任务类型 | 主要指标 | 次要指标 |
|---------|---------|---------|
| 二元分类（通过/失败） | 召回率、精确度、F1 | Cohen's κ |
| 序数量表（1-5评分） | Spearman's ρ、Kendall's τ | Cohen's κ（加权） |
| 成对偏好 | 一致性率、位置一致性 | 置信度校准 |
| 多标签 | 宏观F1、微观F1 | 每个标签的精确度/召回率 |

关键洞察：绝对一致性的高度不如系统性分歧模式重要。与人类在特定标准上系统性不同意的判官比随机噪声更有问题。

## 评估方法

### 直接评分实现

直接评分需要三个组件：清晰的标准、校准的量表和结构化的输出格式。

**标准定义模式**：
```
标准名称：[名称]
描述：[这个标准衡量什么]
权重：[相对重要性，0-1]
```

**量表校准**：
- 1-3量表：二元加中立选项，认知负荷最低
- 1-5量表：标准李克特量表，粒度和可靠性的良好平衡
- 1-10量表：粒度高但难以校准，仅在有详细评分标准时使用

**直接评分的提示词结构**：
```
你是一个专家评估者，评估响应质量。

## 任务
根据每个标准评估以下响应。

## 原始提示
{prompt}

## 要评估的响应
{response}

## 标准
{对于每个标准：名称、描述、权重}

## 说明
对于每个标准：
1. 在响应中找到具体证据
2. 根据评分标准评分（1-{最大}量表）
3. 用证据为你的评分辩护
4. 提出一个具体的改进建议

## 输出格式
用包含评分、辩护和摘要的结构化JSON响应。
```

**思维链要求**：所有评分提示都必须在评分前要求证明。研究表明这比评分优先的方法可靠性提高15-25%。

### 成对比较实现

成对比较本质上对基于偏好的评估更可靠，但需要偏差缓解。

**位置偏差缓解协议**：
1. 第一次：响应A在首位，响应B在其次
2. 第二次：响应B在首位，响应A在其次
3. 一致性检查：如果两次不同意，返回同分且置信度降低
4. 最终判决：与平均置信度一致的赢家

**成对比较的提示词结构**：
```
你是一个专家评估者，比较两个AI响应。

## 关键说明
- 不要因为响应更长就偏好它
- 不要根据位置（第一个或第二个）偏好响应
- 仅根据指定的标准关注质量
- 当响应确实相当时，平局是可接受的

## 原始提示
{prompt}

## 响应A
{response_a}

## 响应B
{response_b}

## 比较标准
{标准列表}

## 说明
1. 首先独立分析每个响应
2. 在每个标准上比较它们
3. 确定总体赢家和置信水平

## 输出格式
包含每个标准的比较、总体赢家、置信度（0-1）和推理的JSON。
```

**置信度校准**：置信度评分应该反映位置一致性：
- 两次都同意：置信度 = 个人置信度的平均值
- 两次不同意：置信度 = 0.5，判决 = 同分

### 评分标准生成

定义良好的评分标准将评估差异减少40-60%，相比开放式评分。

**评分标准组件**：
1. **级别描述**：每个评分级别的清晰边界
2. **特征**：定义每个级别的可观察特征
3. **示例**：每个级别的代表文本（可选但有价值）
4. **边界情况**：模糊情况的指导
5. **评分指南**：一致应用的一般原则

**严格程度校准**：
- **宽松**：降低通过评分的门槛，适合鼓励迭代
- **平衡**：公平，典型的生产使用预期
- **严格**：高标准，适合安全关键或高风险评估

**领域适配**：评分标准应使用领域特定的术语。一个"代码可读性"评分标准提到变量、函数和注释。一个"医学准确性"评分标准引用临床术语和证据标准。

## 实践指导

### 评估流程设计

生产评估系统需要多层：

```
┌─────────────────────────────────────────────────┐
│                 评估流程                        │
├─────────────────────────────────────────────────┤
│                                                   │
│  输入：响应 + 提示 + 上下文                       │
│        │                                        │
│        ▼                                        │
│  ┌─────────────────────┐                        │
│  │   标准加载器        │ ◄── 评分标准，权重      │
│  └──────────┬──────────┘                        │
│             │                                    │
│             ▼                                    │
│  ┌─────────────────────┐                        │
│  │   主评分器          │ ◄── 直接或成对          │
│  └──────────┬──────────┘                        │
│             │                                    │
│             ▼                                    │
│  ┌─────────────────────┐                        │
│  │   偏差缓解          │ ◄── 位置交换等          │
│  └──────────┬──────────┘                        │
│             │                                    │
│             ▼                                    │
│  ┌─────────────────────┐                        │
│  │   置信度评分        │ ◄── 校准                │
│  └──────────┬──────────┘                        │
│             │                                    │
│             ▼                                    │
│  输出：评分 + 辩护 + 置信度                      │
│                                                   │
└─────────────────────────────────────────────────┘
```

### 常见反模式

**反模式：没有辩护的评分**
- 问题：评分缺乏基础，难以调试或改进
- 解决方案：始终在评分前要求基于证据的辩护

**反模式：单次成对比较**
- 问题：位置偏差腐蚀结果
- 解决方案：始终交换位置并检查一致性

**反模式：超载的标准**
- 问题：测量多个事物的标准不可靠
- 解决方案：一个标准 = 一个可测量的方面

**反模式：缺少边界情况指导**
- 问题：评估者处理模糊情况不一致
- 解决方案：在评分标准中包含边界情况和明确的指导

**反模式：忽视置信度校准**
- 问题：高置信度的错误判断比低置信度更糟
- 解决方案：将置信度校准到位置一致性和证据强度

### 决策框架：直接 vs. 成对

使用这个决策树：

```
是否有客观的基准真值？
├── 是 → 直接评分
│   └── 示例：事实准确性、指令遵循、格式合规
│
└── 否 → 是否是偏好或质量判断？
    ├── 是 → 成对比较
    │   └── 示例：语气、风格、说服力、创意
    │
    └── 否 → 考虑参考基础评估
        └── 示例：摘要（与源比较）、翻译（与参考比较）
```

### 扩展评估

对于大容量评估：

1. **LLM评委小组（PoLL）**：使用多个模型作为判官，汇总投票
   - 减少个别模型偏差
   - 虽然成本更高但对高风险决策更可靠

2. **分层评估**：快速便宜的模型进行筛选，昂贵的模型处理边界情况
   - 大容量成本效益
   - 需要筛选阈值的校准

3. **人工在线**：自动化评估明确情况，人工审查低置信度情况
   - 关键应用的最佳可靠性
   - 设计反馈循环来改进自动化评估

## 示例

### 示例1：准确性的直接评分

**输入**：
```
提示："地球季节是什么原因？"
响应："季节是由地球倾斜的轴线引起的。当地球绕太阳公转时，
不同的半球在一年的不同时间接受更直接的阳光。"
标准：事实准确性（权重：1.0）
量表：1-5
```

**输出**：
```json
{
  "criterion": "事实准确性",
  "score": 5,
  "evidence": [
    "正确识别轴向倾斜作为主要原因",
    "正确解释不同半球的不同阳光",
    "不存在事实错误"
  ],
  "justification": "响应准确解释了季节的原因，具有正确的科学推理。轴向倾斜及其对阳光分布的影响都被正确描述。",
  "improvement": "可以添加具体的倾斜角度（23.5°）以更加完整。"
}
```

### 示例2：位置交换的成对比较

**输入**：
```
提示："向初学者解释机器学习"
响应A：[包含行话的技术解释]
响应B：[基于类比的简单解释]
标准：["清晰度", "易接近性"]
```

**第一次（A首先）**：
```json
{ "winner": "B", "confidence": 0.8 }
```

**第二次（B首先）**：
```json
{ "winner": "A", "confidence": 0.6 }
```
（注：赢家是A因为B在首位）

**映射第二次**：
```json
{ "winner": "B", "confidence": 0.6 }
```

**最终结果**：
```json
{
  "winner": "B",
  "confidence": 0.7,
  "positionConsistency": {
    "consistent": true,
    "firstPassWinner": "B",
    "secondPassWinner": "B"
  }
}
```

### 示例3：评分标准生成

**输入**：
```
标准名称："代码可读性"
标准描述："代码的易理解和可维护程度"
领域："软件工程"
量表："1-5"
严格程度："平衡"
```

**输出**（缩略版）：
```json
{
  "levels": [
    {
      "score": 1,
      "label": "较差",
      "description": "代码难以理解，需要大量努力",
      "characteristics": [
        "没有有意义的变量或函数名称",
        "没有注释或文档",
        "嵌套深或逻辑复杂"
      ]
    },
    {
      "score": 3,
      "label": "充分",
      "description": "代码可以理解但需要一些努力",
      "characteristics": [
        "大多数变量有有意义的名称",
        "复杂部分有基本注释",
        "逻辑可以跟踪但可以更清晰"
      ]
    },
    {
      "score": 5,
      "label": "优秀",
      "description": "代码立即清晰且易于维护",
      "characteristics": [
        "所有名称都有描述性且一致",
        "全面的文档",
        "干净、模块化的结构"
      ]
    }
  ],
  "edgeCases": [
    {
      "situation": "代码结构良好但使用领域特定的缩写",
      "guidance": "根据领域专家的可读性评分，而不是一般受众"
    }
  ]
}
```

## 指南

1. **始终在评分前要求证明** - 思维链提示将可靠性提高15-25%

2. **始终在成对比较中交换位置** - 单次比较被位置偏差腐蚀

3. **将量表粒度与评分标准特异性相匹配** - 没有详细的级别描述不要使用1-10

4. **分离客观和主观标准** - 使用直接评分对客观，成对对主观

5. **包含置信度评分** - 校准到位置一致性和证据强度

6. **明确定义边界情况** - 模糊情况导致最多的评估差异

7. **使用领域特定的评分标准** - 通用评分标准产生通用（较少有用）的评估

8. **根据人类判断验证** - 自动化评估仅在与人类评估相关联时有价值

9. **监控系统性偏差** - 按标准、响应类型、模型追踪分歧模式

10. **设计用于迭代** - 评估系统通过反馈循环改进

## 集成

本技能集成了：

- **context-fundamentals** - 评估提示需要有效的上下文结构
- **tool-design** - 评估工具需要适当的模式和错误处理
- **context-optimization** - 评估提示可以优化用于令牌效率
- **evaluation**（基础） - 本技能扩展了基础评估概念

## 参考文献

内部参考：
- [LLM作为判官实现模式](./references/implementation-patterns.md)
- [偏差缓解技术](./references/bias-mitigation.md)
- [指标选择指南](./references/metrics-guide.md)

外部研究：
- [Eugene Yan: 评估LLM评估者的有效性](https://eugeneyan.com/writing/llm-evaluators/)
- [判断LLM作为判官（Zheng等，2023）](https://arxiv.org/abs/2306.05685)
- [G-Eval: 使用GPT-4的NLG评估（Liu等，2023）](https://arxiv.org/abs/2303.16634)
- [大语言模型不是公平的评估者（Wang等，2023）](https://arxiv.org/abs/2305.17926)

此集合中的相关技能：
- evaluation - 基础评估概念
- context-fundamentals - 评估提示的上下文结构
- tool-design - 构建评估工具

---

## 技能元数据

**创建时间**：2024-12-24
**最后更新**：2024-12-24
**作者**：Muratcan Koylan
**版本**：1.0.0

