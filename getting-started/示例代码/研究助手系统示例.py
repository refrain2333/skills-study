"""
å®Œæ•´å®ä¾‹ï¼šæ„å»ºä¸€ä¸ªæ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿ

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ç‹¬ç«‹ä½¿ç”¨ Agent Skills ä¸­çš„åŸç†æ¥æ„å»ºä¸€ä¸ªå®Œæ•´çš„å¤šä»£ç†ç³»ç»Ÿã€‚

ä¸ä¾èµ– Claudeã€Cursor æˆ–ä»»ä½•ç‰¹å®šå¹³å° - åªä½¿ç”¨æ ‡å‡† Python å’Œå¯é…ç½®çš„ LLM APIã€‚

åŠŸèƒ½ï¼š
- æ¥å—ç ”ç©¶è¯é¢˜
- åˆ†è§£ä¸ºå­ä»»åŠ¡
- å¹¶è¡Œæ‰§è¡Œæœç´¢å’Œåˆ†æ
- ç»¼åˆç”ŸæˆæŠ¥å‘Š
- æŒä¹…åŒ–å†…å­˜å’Œç»“æœ
"""

import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict, field
import sys


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒæ•°æ®ç»“æ„
# ============================================================================

@dataclass
class Task:
    """ä»£è¡¨ä¸€ä¸ªä»»åŠ¡"""
    id: str
    title: str
    description: str
    task_type: str  # "search", "analyze", "synthesize"
    status: str = "pending"  # "pending", "in_progress", "completed", "failed"
    result: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None
    created_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None


@dataclass
class Message:
    """ä»£ç†é—´é€šä¿¡æ¶ˆæ¯"""
    sender: str
    receiver: str
    content: Dict[str, Any]
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class ResearchResult:
    """ç ”ç©¶ç»“æœ"""
    topic: str
    search_results: List[Dict] = field(default_factory=list)
    analysis: Dict[str, Any] = field(default_factory=dict)
    synthesis: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šç®€åŒ–çš„ LLM æ¥å£ï¼ˆå¯é…ç½®ä½¿ç”¨ä»»ä½• APIï¼‰
# ============================================================================

class LLMInterface:
    """
    LLM æ¥å£ - è¿™æ˜¯ä¸å…·ä½“ LLM API çš„é›†æˆç‚¹
    
    ä½¿ç”¨æ­¥éª¤ï¼š
    1. æ›¿æ¢ call() æ–¹æ³•ä¸­çš„å®ç°
    2. æ”¯æŒä»»ä½• APIï¼šOpenAI, Anthropic, æœ¬åœ° Ollama, ç­‰
    """
    
    def __init__(self, api_key: Optional[str] = None, model: str = "default"):
        self.api_key = api_key
        self.model = model
        self.call_count = 0
        self.total_tokens = 0
    
    def call(self, prompt: str, system_prompt: str = None, 
             temperature: float = 0.7, max_tokens: int = 2000) -> str:
        """
        è°ƒç”¨ LLM çš„é€šç”¨æ¥å£ã€‚
        
        TODO: æ ¹æ®ä½ çš„ LLM æä¾›å•†æ›¿æ¢å®ç°
        """
        self.call_count += 1
        
        # ç¤ºä¾‹å®ç°ï¼šè¿™é‡Œåº”è¯¥è°ƒç”¨ä½ çš„ LLM API
        # 
        # é€‰æ‹©ä½ çš„æä¾›å•†ï¼š
        # 
        # === OpenAI ===
        # from openai import OpenAI
        # client = OpenAI(api_key=self.api_key)
        # response = client.chat.completions.create(...)
        # 
        # === Anthropic ===
        # from anthropic import Anthropic
        # client = Anthropic(api_key=self.api_key)
        # response = client.messages.create(...)
        # 
        # === æœ¬åœ° Ollama ===
        # import requests
        # response = requests.post("http://localhost:11434/api/generate", ...)
        
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬è¿”å›æ¨¡æ‹Ÿå“åº”
        if "search" in system_prompt.lower() or "æœç´¢" in prompt:
            return self._mock_search_response()
        elif "analyze" in system_prompt.lower() or "åˆ†æ" in prompt:
            return self._mock_analysis_response()
        else:
            return self._mock_synthesis_response()
    
    def _mock_search_response(self) -> str:
        """æ¨¡æ‹Ÿæœç´¢ç»“æœ"""
        return """### æœç´¢ç»“æœ

1. **äººå·¥æ™ºèƒ½çš„æœªæ¥å±•æœ›** (2024å¹´)
   - å…³é”®è¯: æ·±åº¦å­¦ä¹ , è½¬æ¢å™¨, å¯æ‰©å±•æ€§
   - è¦ç‚¹: æ¨¡å‹è§„æ¨¡æŒç»­å¢å¤§ï¼Œä½†æ•ˆç‡æ”¹è¿›ç©ºé—´æœ‰é™

2. **æœºå™¨å­¦ä¹ å®‰å…¨æ€§** (2024å¹´)
   - å…³é”®è¯: å¯¹æŠ—æ€§æ ·æœ¬, é²æ£’æ€§, éªŒè¯
   - è¦ç‚¹: å®‰å…¨æ€§æ˜¯ç”Ÿäº§éƒ¨ç½²çš„å…³é”®è€ƒè™‘

3. **æ™ºèƒ½ä½“æ¶æ„è®¾è®¡** (2024å¹´)
   - å…³é”®è¯: å¤šä»£ç†ç³»ç»Ÿ, åè°ƒ, ä¸Šä¸‹æ–‡ç®¡ç†
   - è¦ç‚¹: åˆ†å¸ƒå¼æ¶æ„æ”¹å–„äº†ç³»ç»Ÿæ‰©å±•æ€§"""
    
    def _mock_analysis_response(self) -> str:
        """æ¨¡æ‹Ÿåˆ†æç»“æœ"""
        return """### åˆ†æç»“æœ

**æ ¸å¿ƒä¸»é¢˜ï¼š**
- äººå·¥æ™ºèƒ½èƒ½åŠ›çš„å¢é•¿ä¸é™åˆ¶çš„å¹³è¡¡
- å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§è€ƒè™‘
- ç³»ç»Ÿæ¶æ„å¯¹æ€§èƒ½çš„å½±å“

**å…³é”®è¶‹åŠ¿ï¼š**
1. ä»è§„æ¨¡å¢é•¿åˆ°æ•ˆç‡ä¼˜åŒ–çš„è½¬å˜
2. ä»å•ä½“æ¨¡å‹åˆ°åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ¼”è¿›
3. ä»ç†è®ºç ”ç©¶åˆ°å®é™…éƒ¨ç½²çš„å…³æ³¨

**çŸ¥è¯†ç¼ºå£ï¼š**
- é•¿æœŸå¯æŒç»­æ€§çš„é—®é¢˜
- èµ„æºæ•ˆç‡ä¼˜åŒ–çš„æ–¹æ³•"""
    
    def _mock_synthesis_response(self) -> str:
        """æ¨¡æ‹Ÿç»¼åˆç»“æœ"""
        return """### ç ”ç©¶ç»¼åˆæŠ¥å‘Š

## æ‘˜è¦
å½“å‰äººå·¥æ™ºèƒ½ç ”ç©¶æ­£å¤„äºä»è§„æ¨¡å¯¼å‘è½¬å‘æ•ˆç‡å’Œå®‰å…¨å¯¼å‘çš„è½¬æŠ˜ç‚¹ã€‚

## ä¸»è¦å‘ç°
1. **æŠ€æœ¯è¿›æ­¥** - è™½ç„¶æ¨¡å‹è§„æ¨¡ç»§ç»­å¢å¤§ï¼Œä½†æ”¹è¿›æ•ˆç›Šåœ¨é€’å‡
2. **å®‰å…¨å…³æ³¨** - ç”Ÿäº§éƒ¨ç½²è¶Šæ¥è¶Šé‡è§†é²æ£’æ€§å’Œå®‰å…¨æ€§
3. **æ¶æ„åˆ›æ–°** - å¤šä»£ç†å’Œåˆ†å¸ƒå¼ç³»ç»Ÿæä¾›äº†æ–°çš„æ‰©å±•è·¯å¾„

## å»ºè®®
- æŠ•èµ„äºæ•ˆç‡ä¼˜åŒ–è€Œä¸ä»…ä»…æ˜¯è§„æ¨¡å¢åŠ 
- å»ºç«‹å®‰å…¨è¯„ä¼°ä¸ºæ ‡å‡†æµç¨‹
- æ¢ç´¢åˆ†å¸ƒå¼å’Œå¤šä»£ç†æ¶æ„

## ç»“è®º
æœªæ¥çš„ AI ç³»ç»Ÿå°†æ›´åŠ é‡è§†å¯é æ€§ã€æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯æ€§èƒ½æŒ‡æ ‡ã€‚"""


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šä»£ç†å®ç°
# ============================================================================

class Agent:
    """ä»£ç†åŸºç±»"""
    
    def __init__(self, name: str, llm: LLMInterface, agent_type: str = "general"):
        self.name = name
        self.llm = llm
        self.agent_type = agent_type
        self.memory: List[Dict] = []
        self.inbox: List[Message] = []
    
    def add_memory(self, content: str, category: str = "general"):
        """æ·»åŠ å†…å­˜"""
        self.memory.append({
            "timestamp": datetime.now().isoformat(),
            "category": category,
            "content": content
        })
    
    def receive_message(self, message: Message):
        """æ¥æ”¶æ¶ˆæ¯"""
        self.inbox.append(message)
    
    def process_messages(self) -> List[Message]:
        """å¤„ç†æ‰€æœ‰æ”¶åˆ°çš„æ¶ˆæ¯"""
        responses = []
        for msg in self.inbox:
            response = self._respond_to_message(msg)
            if response:
                responses.append(response)
        self.inbox = []  # æ¸…ç©ºæ”¶ä»¶ç®±
        return responses
    
    def _respond_to_message(self, message: Message) -> Optional[Message]:
        """å“åº”å•ä¸ªæ¶ˆæ¯"""
        # å­ç±»åº”è¯¥è¦†ç›–è¿™ä¸ªæ–¹æ³•
        return None


class SearchAgent(Agent):
    """æœç´¢ä»£ç† - æœç´¢ç›¸å…³ä¿¡æ¯"""
    
    def __init__(self, name: str, llm: LLMInterface):
        super().__init__(name, llm, "searcher")
        self.search_history = []
    
    def search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡Œæœç´¢"""
        print(f"ğŸ” {self.name} æœç´¢: {query}")
        
        system_prompt = """You are a research search agent. 
        Your task is to find and summarize relevant information about the given topic.
        Provide structured search results with sources and key findings."""
        
        response = self.llm.call(query, system_prompt=system_prompt)
        
        result = {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "content": response,
            "status": "completed"
        }
        
        self.search_history.append(result)
        self.add_memory(f"æœç´¢äº†: {query}", category="search")
        
        return result


class AnalysisAgent(Agent):
    """åˆ†æä»£ç† - åˆ†ææœç´¢ç»“æœ"""
    
    def __init__(self, name: str, llm: LLMInterface):
        super().__init__(name, llm, "analyst")
        self.analysis_history = []
    
    def analyze(self, content: str, analysis_type: str = "general") -> Dict[str, Any]:
        """æ‰§è¡Œåˆ†æ"""
        print(f"ğŸ“Š {self.name} åˆ†æ: {analysis_type}")
        
        system_prompt = """You are a research analysis agent.
        Your task is to analyze the provided content and extract key insights,
        patterns, and relationships. Provide structured analysis."""
        
        prompt = f"""åˆ†æç±»å‹: {analysis_type}

å†…å®¹:
{content}

è¯·æä¾›è¯¦ç»†çš„åˆ†æç»“æœã€‚"""
        
        response = self.llm.call(prompt, system_prompt=system_prompt)
        
        result = {
            "analysis_type": analysis_type,
            "timestamp": datetime.now().isoformat(),
            "content": response,
            "status": "completed"
        }
        
        self.analysis_history.append(result)
        self.add_memory(f"åˆ†æäº†: {analysis_type}", category="analysis")
        
        return result


class SynthesisAgent(Agent):
    """ç»¼åˆä»£ç† - ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    
    def __init__(self, name: str, llm: LLMInterface):
        super().__init__(name, llm, "synthesizer")
        self.reports = []
    
    def synthesize(self, search_results: Dict, analysis_results: Dict) -> Dict[str, Any]:
        """ç»¼åˆæ‰€æœ‰ç»“æœç”ŸæˆæŠ¥å‘Š"""
        print(f"ğŸ“ {self.name} ç»¼åˆæŠ¥å‘Š")
        
        system_prompt = """You are a research synthesis agent.
        Your task is to combine search results and analysis into a comprehensive report.
        Create a well-structured, coherent research report."""
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆç»¼åˆç ”ç©¶æŠ¥å‘Š:

æœç´¢ç»“æœ:
{json.dumps(search_results, ensure_ascii=False, indent=2)}

åˆ†æç»“æœ:
{json.dumps(analysis_results, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆä¸€ä»½å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…æ‹¬æ‘˜è¦ã€ä¸»è¦å‘ç°ã€å»ºè®®å’Œç»“è®ºã€‚"""
        
        response = self.llm.call(prompt, system_prompt=system_prompt)
        
        result = {
            "timestamp": datetime.now().isoformat(),
            "content": response,
            "status": "completed"
        }
        
        self.reports.append(result)
        self.add_memory("ç”Ÿæˆäº†ç»¼åˆæŠ¥å‘Š", category="synthesis")
        
        return result


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šåè°ƒå™¨ï¼ˆç›‘ç£è€…æ¨¡å¼ï¼‰
# ============================================================================

class ResearchCoordinator:
    """ç ”ç©¶åè°ƒå™¨ - ç›‘ç£è€…ä»£ç†"""
    
    def __init__(self, llm: LLMInterface):
        self.llm = llm
        self.search_agent = SearchAgent("SearchAgent", llm)
        self.analysis_agent = AnalysisAgent("AnalysisAgent", llm)
        self.synthesis_agent = SynthesisAgent("SynthesisAgent", llm)
        
        self.task_history: List[Task] = []
        self.results_history: List[ResearchResult] = []
    
    def decompose_task(self, topic: str) -> List[Task]:
        """å°†ç ”ç©¶è¯é¢˜åˆ†è§£ä¸ºå­ä»»åŠ¡"""
        print(f"\nğŸ“‹ åˆ†è§£ä»»åŠ¡: {topic}")
        
        tasks = [
            Task(
                id="search_001",
                title="æœç´¢ç›¸å…³ä¿¡æ¯",
                description=f"æœç´¢å…³äº {topic} çš„ç›¸å…³ä¿¡æ¯",
                task_type="search"
            ),
            Task(
                id="analyze_001",
                title="åˆ†ææœç´¢ç»“æœ",
                description=f"åˆ†ææœç´¢ç»“æœä¸­çš„æ¨¡å¼å’Œè¶‹åŠ¿",
                task_type="analyze"
            ),
            Task(
                id="synthesize_001",
                title="ç”Ÿæˆç»¼åˆæŠ¥å‘Š",
                description=f"ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
                task_type="synthesize"
            )
        ]
        
        self.task_history.extend(tasks)
        return tasks
    
    def execute_task(self, task: Task, context: Dict = None) -> Task:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        task.status = "in_progress"
        print(f"  â–¶ï¸  æ‰§è¡Œ: {task.title}")
        
        try:
            if task.task_type == "search":
                task.result = self.search_agent.search(task.description)
            elif task.task_type == "analyze":
                search_results = context.get("search_results", "")
                task.result = self.analysis_agent.analyze(search_results)
            elif task.task_type == "synthesize":
                task.result = self.synthesis_agent.synthesize(
                    context.get("search_results", {}),
                    context.get("analysis_results", {})
                )
            
            task.status = "completed"
            task.completed_at = datetime.now()
            
        except Exception as e:
            task.status = "failed"
            task.error = str(e)
            print(f"    âŒ é”™è¯¯: {e}")
        
        return task
    
    def research(self, topic: str) -> ResearchResult:
        """æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶æµç¨‹"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹ç ”ç©¶: {topic}")
        print(f"{'='*60}")
        
        # ç¬¬ä¸€æ­¥ï¼šåˆ†è§£ä»»åŠ¡
        tasks = self.decompose_task(topic)
        
        # ç¬¬äºŒæ­¥ï¼šæŒ‰é¡ºåºæ‰§è¡Œä»»åŠ¡
        context = {"topic": topic}
        
        for task in tasks:
            task = self.execute_task(task, context)
            
            # å°†ç»“æœå­˜å‚¨åˆ°ä¸Šä¸‹æ–‡ä¸­ä¾›åç»­ä»»åŠ¡ä½¿ç”¨
            if task.status == "completed":
                if task.task_type == "search":
                    context["search_results"] = task.result
                elif task.task_type == "analyze":
                    context["analysis_results"] = task.result
        
        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæœ€ç»ˆç»“æœå¯¹è±¡
        result = ResearchResult(
            topic=topic,
            search_results=[context.get("search_results", {})],
            analysis=context.get("analysis_results", {}),
            synthesis=tasks[-1].result if len(tasks) > 2 else {},
            metadata={
                "total_tasks": len(tasks),
                "completed_tasks": sum(1 for t in tasks if t.status == "completed"),
                "timestamp": datetime.now().isoformat(),
                "llm_calls": self.llm.call_count
            }
        )
        
        self.results_history.append(result)
        
        return result
    
    def save_results(self, result: ResearchResult, output_dir: str = "./research_output"):
        """ä¿å­˜ç ”ç©¶ç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"research_{timestamp}.json"
        filepath = output_path / filename
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        result_dict = {
            "topic": result.topic,
            "search_results": result.search_results,
            "analysis": result.analysis,
            "synthesis": result.synthesis,
            "metadata": result.metadata
        }
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        
        return filepath
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_tasks = len(self.task_history)
        completed = sum(1 for t in self.task_history if t.status == "completed")
        failed = sum(1 for t in self.task_history if t.status == "failed")
        
        return {
            "total_researches": len(self.results_history),
            "total_tasks": total_tasks,
            "completed_tasks": completed,
            "failed_tasks": failed,
            "success_rate": f"{(completed/max(1, total_tasks)*100):.1f}%",
            "llm_calls": self.llm.call_count,
            "search_agent_memories": len(self.search_agent.memory),
            "analysis_agent_memories": len(self.analysis_agent.memory),
            "synthesis_agent_memories": len(self.synthesis_agent.memory)
        }


# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»ç¨‹åº"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿ - Agent Skills å®æˆ˜ç¤ºä¾‹                   â•‘
â•‘                                                                â•‘
â•‘  æ¼”ç¤ºå¦‚ä½•ç‹¬ç«‹æ„å»ºå¤šä»£ç†ç³»ç»Ÿï¼Œä¸ä¾èµ– Claude æˆ– Cursor         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # åˆå§‹åŒ– LLMï¼ˆä½¿ç”¨é»˜è®¤ API keyï¼Œç”Ÿäº§ç¯å¢ƒåº”è¯¥ç”¨ç¯ä¿å˜é‡ï¼‰
    llm = LLMInterface(
        api_key="your-api-key-here",
        model="default"
    )
    
    # åˆ›å»ºåè°ƒå™¨
    coordinator = ResearchCoordinator(llm)
    
    # æ‰§è¡Œç ”ç©¶ä»»åŠ¡
    research_topics = [
        "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘",
        "å¤šä»£ç†ç³»ç»Ÿçš„æ¶æ„æ¨¡å¼",
    ]
    
    for topic in research_topics:
        try:
            # æ‰§è¡Œç ”ç©¶
            result = coordinator.research(topic)
            
            # ä¿å­˜ç»“æœ
            coordinator.save_results(result)
            
            # æ˜¾ç¤ºç»¼åˆæŠ¥å‘Š
            print(f"\nğŸ“„ ç ”ç©¶æŠ¥å‘Šæ‘˜è¦:")
            print("â”€" * 60)
            if result.synthesis:
                print(result.synthesis.get("content", "N/A")[:500] + "...")
            print("â”€" * 60)
            
        except Exception as e:
            print(f"âŒ ç ”ç©¶å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡:")
    print("â”€" * 60)
    stats = coordinator.get_statistics()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    print("â”€" * 60)
    
    print(f"\nâœ¨ ç ”ç©¶å®Œæˆï¼")


if __name__ == "__main__":
    main()

