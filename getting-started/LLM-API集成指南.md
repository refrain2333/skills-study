# LLM API é›†æˆæŒ‡å—

è¿™ä¸ªæŒ‡å—å±•ç¤ºäº†å¦‚ä½•å°†ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿä¸ä¸åŒçš„ LLM æä¾›å•†é›†æˆã€‚

## å¿«é€Ÿé€‰æ‹©

é€‰æ‹©ä½ æƒ³è¦ä½¿ç”¨çš„ LLM æä¾›å•†ï¼š

- **OpenAI (GPT-4o)** - æœ€å¼ºå¤§çš„é—­æºæ¨¡å‹
- **Anthropic (Claude)** - å®‰å…¨æ€§å’Œæ¨ç†èƒ½åŠ›å¼º
- **æœ¬åœ° Ollama** - å®Œå…¨ç§å¯†ï¼Œæ— éœ€ API key
- **LangChain** - ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒå¤šä¸ªæä¾›å•†
- **Groq** - è¶…å¿«æ¨ç†é€Ÿåº¦ï¼ˆé€‚åˆå®æ—¶åº”ç”¨ï¼‰

---

## æ–¹æ¡ˆ 1ï¼šOpenAI (GPT-4o)

### æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ–

```bash
pip install openai
```

### æ­¥éª¤ 2ï¼šè®¾ç½® API Key

```bash
# Linux/Mac
export OPENAI_API_KEY="sk-xxxxx"

# Windows (PowerShell)
$env:OPENAI_API_KEY="sk-xxxxx"
```

### æ­¥éª¤ 3ï¼šé›†æˆä»£ç 

åœ¨ `ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿç¤ºä¾‹.py` ä¸­æ›¿æ¢ `LLMInterface` ç±»çš„ `call()` æ–¹æ³•ï¼š

```python
def call(self, prompt: str, system_prompt: str = None, 
         temperature: float = 0.7, max_tokens: int = 2000) -> str:
    """è°ƒç”¨ OpenAI API"""
    from openai import OpenAI
    
    client = OpenAI(api_key=self.api_key)
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    response = client.chat.completions.create(
        model=self.model or "gpt-4o",
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens
    )
    
    # è®°å½•ä»¤ç‰Œä½¿ç”¨
    self.total_tokens += response.usage.total_tokens
    
    return response.choices[0].message.content
```

### æ­¥éª¤ 4ï¼šè¿è¡Œ

```bash
python ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿç¤ºä¾‹.py
```

### æˆæœ¬ä¼°ç®—

- è¾“å…¥ï¼š$0.003/1K tokensï¼ˆgpt-4oï¼‰
- è¾“å‡ºï¼š$0.012/1K tokensï¼ˆgpt-4oï¼‰
- å…¸å‹ç ”ç©¶æŸ¥è¯¢ï¼š50K tokens â‰ˆ $0.70

---

## æ–¹æ¡ˆ 2ï¼šAnthropic Claude

### æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ–

```bash
pip install anthropic
```

### æ­¥éª¤ 2ï¼šè®¾ç½® API Key

```bash
export ANTHROPIC_API_KEY="sk-ant-xxxxx"
```

### æ­¥éª¤ 3ï¼šé›†æˆä»£ç 

```python
def call(self, prompt: str, system_prompt: str = None, 
         temperature: float = 0.7, max_tokens: int = 2000) -> str:
    """è°ƒç”¨ Anthropic Claude API"""
    from anthropic import Anthropic
    
    client = Anthropic(api_key=self.api_key)
    
    response = client.messages.create(
        model=self.model or "claude-3-5-sonnet-20241022",
        max_tokens=max_tokens,
        temperature=temperature,
        system=system_prompt or "You are a helpful assistant.",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    
    # è®°å½•ä»¤ç‰Œä½¿ç”¨
    self.total_tokens += response.usage.input_tokens + response.usage.output_tokens
    
    return response.content[0].text
```

### æˆæœ¬ä¼°ç®—

- Claude 3.5 Sonnetï¼š$0.003/1K input, $0.015/1K output
- å…¸å‹ç ”ç©¶æŸ¥è¯¢ï¼š50K tokens â‰ˆ $0.99

---

## æ–¹æ¡ˆ 3ï¼šæœ¬åœ° Ollamaï¼ˆæ¨èç”¨äºéšç§å’Œå¼€å‘ï¼‰

### æ­¥éª¤ 1ï¼šå®‰è£… Ollama

è®¿é—® https://ollama.ai å¹¶ä¸‹è½½å®‰è£…ç¨‹åº

### æ­¥éª¤ 2ï¼šå¯åŠ¨ Ollama æœåŠ¡

```bash
ollama serve

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯æ‹‰å–æ¨¡å‹
ollama pull llama2          # 7Bï¼Œå¿«é€Ÿ
ollama pull mistral         # 7Bï¼Œé«˜è´¨é‡
ollama pull neural-chat     # é’ˆå¯¹å¯¹è¯ä¼˜åŒ–
```

### æ­¥éª¤ 3ï¼šé›†æˆä»£ç 

```python
def call(self, prompt: str, system_prompt: str = None, 
         temperature: float = 0.7, max_tokens: int = 2000) -> str:
    """è°ƒç”¨æœ¬åœ° Ollama API"""
    import requests
    import json
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    # æ„å»ºè¯·æ±‚
    payload = {
        "model": self.model or "mistral",
        "messages": messages,
        "temperature": temperature,
        "stream": False
    }
    
    try:
        response = requests.post(
            "http://localhost:11434/api/chat",
            json=payload,
            timeout=300  # æœ¬åœ°å¯èƒ½è¾ƒæ…¢
        )
        response.raise_for_status()
        
        result = response.json()
        return result["message"]["content"]
    
    except requests.exceptions.ConnectionError:
        raise ConnectionError(
            "æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ã€‚"
            "è¯·ç¡®ä¿å·²è¿è¡Œ `ollama serve`"
        )
```

### ä¼˜åŠ¿å’ŒåŠ£åŠ¿

âœ… **ä¼˜åŠ¿ï¼š**
- å®Œå…¨å…è´¹
- æ— éšç§é¡¾è™‘
- æœ¬åœ°è¿è¡Œï¼Œç¦»çº¿å·¥ä½œ
- æ— APIé™æµ

âŒ **åŠ£åŠ¿ï¼š**
- éœ€è¦æœ¬åœ°è®¡ç®—èµ„æº
- æ¨¡å‹è´¨é‡é€šå¸¸ä½äºå•†ä¸šæ¨¡å‹
- éœ€è¦æ›´å¤šçš„æç¤ºå·¥ç¨‹

---

## æ–¹æ¡ˆ 4ï¼šLangChainï¼ˆç»Ÿä¸€æ¥å£ï¼‰

### æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ–

```bash
pip install langchain langchain-openai langchain-anthropic
```

### æ­¥éª¤ 2ï¼šé›†æˆä»£ç 

```python
from langchain.llms import OpenAI, Anthropic
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

class LangChainLLMInterface(LLMInterface):
    def __init__(self, provider: str = "openai", api_key: str = None, model: str = None):
        super().__init__(api_key, model)
        
        if provider == "openai":
            self.llm = OpenAI(
                api_key=api_key,
                model=model or "gpt-4o",
                temperature=0.7
            )
        elif provider == "anthropic":
            self.llm = Anthropic(
                api_key=api_key,
                model=model or "claude-3-5-sonnet-20241022"
            )
    
    def call(self, prompt: str, system_prompt: str = None, 
             temperature: float = 0.7, max_tokens: int = 2000) -> str:
        """ä½¿ç”¨ LangChain è°ƒç”¨ LLM"""
        
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"
        
        response = self.llm.predict(full_prompt, max_tokens=max_tokens)
        self.call_count += 1
        
        return response
```

### ä½¿ç”¨

```python
# è½»æ¾åˆ‡æ¢æä¾›å•†
llm = LangChainLLMInterface(provider="openai", model="gpt-4o")
# æˆ–
llm = LangChainLLMInterface(provider="anthropic", model="claude-3-5-sonnet-20241022")
```

---

## æ–¹æ¡ˆ 5ï¼šGroqï¼ˆæå¿«æ¨ç†ï¼‰

### æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ–

```bash
pip install groq
```

### æ­¥éª¤ 2ï¼šè®¾ç½® API Key

ä» https://console.groq.com è·å– API Key

```bash
export GROQ_API_KEY="gsk_xxxxx"
```

### æ­¥éª¤ 3ï¼šé›†æˆä»£ç 

```python
def call(self, prompt: str, system_prompt: str = None, 
         temperature: float = 0.7, max_tokens: int = 2000) -> str:
    """è°ƒç”¨ Groq APIï¼ˆè¶…å¿«æ¨ç†ï¼‰"""
    from groq import Groq
    
    client = Groq(api_key=self.api_key)
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    response = client.chat.completions.create(
        model=self.model or "mixtral-8x7b-32768",
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens
    )
    
    self.total_tokens += response.usage.total_tokens
    return response.choices[0].message.content
```

### æ€§èƒ½å¯¹æ¯”

| æä¾›å•† | é€Ÿåº¦ | æˆæœ¬ | è´¨é‡ | éšç§ |
|-------|------|------|------|------|
| Groq | â­â­â­â­â­ | â­â­ | â­â­â­ | â­â­ |
| OpenAI | â­â­â­ | â­â­ | â­â­â­â­â­ | â­ |
| Anthropic | â­â­â­ | â­ | â­â­â­â­â­ | â­â­ |
| Ollama | â­ | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ |

---

## å®Œæ•´çš„å¤šæä¾›å•†åˆ‡æ¢ç¤ºä¾‹

```python
import os
from enum import Enum

class LLMProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    OLLAMA = "ollama"
    GROQ = "groq"
    LANGCHAIN = "langchain"


def create_llm_interface(provider: LLMProvider, model: str = None) -> LLMInterface:
    """å·¥å‚å‡½æ•° - æ ¹æ®ç¯å¢ƒå˜é‡æˆ–å‚æ•°é€‰æ‹© LLM æä¾›å•†"""
    
    api_key = os.getenv(f"{provider.name}_API_KEY")
    
    if provider == LLMProvider.OPENAI:
        return OpenAIInterface(api_key, model or "gpt-4o")
    elif provider == LLMProvider.ANTHROPIC:
        return AnthropicInterface(api_key, model or "claude-3-5-sonnet-20241022")
    elif provider == LLMProvider.OLLAMA:
        return OllamaInterface(model or "mistral")
    elif provider == LLMProvider.GROQ:
        return GroqInterface(api_key, model or "mixtral-8x7b-32768")
    else:
        raise ValueError(f"æœªçŸ¥çš„æä¾›å•†: {provider}")


# ä½¿ç”¨
if __name__ == "__main__":
    # ä»ç¯å¢ƒå˜é‡è¯»å–é€‰æ‹©
    provider_str = os.getenv("LLM_PROVIDER", "ollama").lower()
    provider = LLMProvider[provider_str.upper()]
    
    llm = create_llm_interface(provider)
    coordinator = ResearchCoordinator(llm)
    
    # æ‰§è¡Œç ”ç©¶...
```

### ç¯å¢ƒå˜é‡é…ç½®æ–‡ä»¶ï¼ˆ`.env`ï¼‰

```bash
# é€‰æ‹©æä¾›å•†
LLM_PROVIDER=ollama
# LLM_PROVIDER=openai
# LLM_PROVIDER=anthropic

# API Keys
OPENAI_API_KEY=sk-xxxxx
ANTHROPIC_API_KEY=sk-ant-xxxxx
GROQ_API_KEY=gsk_xxxxx

# æ¨¡å‹é€‰æ‹©
OPENAI_MODEL=gpt-4o
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
GROQ_MODEL=mixtral-8x7b-32768
OLLAMA_MODEL=mistral
```

### è¿è¡Œæ—¶åŠ è½½é…ç½®

```python
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# ç°åœ¨æ‰€æœ‰ç¯å¢ƒå˜é‡éƒ½å¯ç”¨
provider = LLMProvider[os.getenv("LLM_PROVIDER").upper()]
llm = create_llm_interface(provider)
```

---

## æˆæœ¬è¿½è¸ªç¤ºä¾‹

```python
class CostTracker:
    """è·Ÿè¸ª LLM API æˆæœ¬"""
    
    PRICING = {
        "gpt-4o": {"input": 0.003, "output": 0.012},
        "claude-3-5-sonnet": {"input": 0.003, "output": 0.015},
        "mixtral-8x7b": {"input": 0.0, "output": 0.0},  # å…è´¹
        "mistral": {"input": 0.0, "output": 0.0},  # æœ¬åœ°ï¼Œå…è´¹
    }
    
    def __init__(self, model: str):
        self.model = model
        self.input_tokens = 0
        self.output_tokens = 0
    
    def track(self, input_tokens: int, output_tokens: int):
        self.input_tokens += input_tokens
        self.output_tokens += output_tokens
    
    def get_cost(self) -> float:
        if self.model not in self.PRICING:
            return 0.0
        
        pricing = self.PRICING[self.model]
        input_cost = (self.input_tokens / 1000) * pricing["input"]
        output_cost = (self.output_tokens / 1000) * pricing["output"]
        
        return input_cost + output_cost
    
    def report(self) -> str:
        cost = self.get_cost()
        return f"""
æˆæœ¬æŠ¥å‘Š ({self.model}):
  è¾“å…¥ä»¤ç‰Œ: {self.input_tokens:,}
  è¾“å‡ºä»¤ç‰Œ: {self.output_tokens:,}
  æ€»ä»¤ç‰Œ: {self.input_tokens + self.output_tokens:,}
  ä¼°è®¡æˆæœ¬: ${cost:.4f}
        """.strip()
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šè¿æ¥è¶…æ—¶

**ç—‡çŠ¶ï¼š** `ConnectionError: timeout`

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# å¢åŠ è¶…æ—¶æ—¶é—´
response = requests.post(
    url,
    json=payload,
    timeout=300  # å¢åŠ åˆ° 300 ç§’
)
```

### é—®é¢˜ 2ï¼šAPI é™æµ

**ç—‡çŠ¶ï¼š** `RateLimitError`

**è§£å†³æ–¹æ¡ˆï¼š**
```python
import time
from tenacity import retry, wait_exponential

@retry(wait=wait_exponential(multiplier=1, min=2, max=10))
def call_llm_with_retry(prompt):
    # è‡ªåŠ¨é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿
    return llm.call(prompt)
```

### é—®é¢˜ 3ï¼šOllama è¿æ¥å¤±è´¥

**ç—‡çŠ¶ï¼š** `æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡`

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ
ollama serve

# 2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
ollama list

# 3. æ‹‰å–æ¨¡å‹
ollama pull mistral

# 4. æ£€æŸ¥ localhost:11434 æ˜¯å¦å¯è®¿é—®
curl http://localhost:11434/api/tags
```

---

## æ¨èé…ç½®

### å¼€å‘ç¯å¢ƒ

```bash
# å…è´¹ã€å¿«é€Ÿã€ç¦»çº¿
LLM_PROVIDER=ollama
OLLAMA_MODEL=mistral
```

### ç”Ÿäº§ç¯å¢ƒ

```bash
# é«˜è´¨é‡ã€å¯é 
LLM_PROVIDER=openai
OPENAI_MODEL=gpt-4o
```

### å®æ—¶åº”ç”¨

```bash
# è¶…å¿«é€Ÿæ¨ç†
LLM_PROVIDER=groq
GROQ_MODEL=mixtral-8x7b-32768
```

### éšç§æ•æ„Ÿ

```bash
# å®Œå…¨æœ¬åœ°ï¼Œæ— éšç§æ³„éœ²
LLM_PROVIDER=ollama
OLLAMA_MODEL=neural-chat
```

---

## æ€»ç»“

- **é€‰æ‹©æä¾›å•†** - åŸºäºä½ çš„éœ€æ±‚ï¼ˆæˆæœ¬ã€è´¨é‡ã€éšç§ã€é€Ÿåº¦ï¼‰
- **å®ç° `call()` æ–¹æ³•** - æ›¿æ¢ LLMInterface ä¸­çš„é€šç”¨å®ç°
- **è¿½è¸ªæˆæœ¬** - ä»ç¬¬ä¸€å¤©å°±ç›‘æ§å¼€æ”¯
- **ä½¿ç”¨ç¯å¢ƒå˜é‡** - ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç  API keys
- **å®ç°é”™è¯¯å¤„ç†** - é‡è¯•æœºåˆ¶ã€è¶…æ—¶å¤„ç†ã€é™æµå¤„ç†

ç¥ä½ æ„å»ºæˆåŠŸçš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼ğŸš€

