# ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹æŒ‡å—

è¿™ä¸ªæŒ‡å—å°†å¸®åŠ©ä½ åœ¨5åˆ†é’Ÿå†…ç†è§£å¦‚ä½•ä½¿ç”¨ Agent Skills ç‹¬ç«‹æ„å»ºæ™ºèƒ½ä½“é¡¹ç›®ã€‚

---

## ğŸ“Œ ä½ éœ€è¦çŸ¥é“çš„ä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µ

### 1. ä¸Šä¸‹æ–‡æ˜¯æœ‰é™èµ„æº ğŸ’°

```
è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ = æœ‰é™çš„"æ³¨æ„åŠ›é¢„ç®—"

é”™è¯¯è§‚ç‚¹: æ›´å¤§çš„ä¸Šä¸‹æ–‡ = æ›´å¥½çš„ç»“æœ
æ­£ç¡®è§‚ç‚¹: ç²¾å¿ƒé€‰æ‹©çš„ä¿¡æ¯ > å¤§é‡æ— å…³ä¿¡æ¯
```

**å®ä¾‹ï¼š**
```
âŒ ä¸å¥½: åŠ è½½100ä¸ªæœç´¢ç»“æœ
âœ… å¥½: åŠ è½½æœ€ç›¸å…³çš„5ä¸ªç»“æœ
```

### 2. å¤šä»£ç†æ¶æ„éš”ç¦»ä¸Šä¸‹æ–‡ ğŸ¤

```
å•ä»£ç† â†’ ä»£ç†A å¤„ç†æ‰€æœ‰é—®é¢˜ï¼ˆå®¹æ˜“è¶…è½½ï¼‰
å¤šä»£ç† â†’ ä»£ç†Aï¼ˆæœç´¢ï¼‰+ ä»£ç†Bï¼ˆåˆ†æï¼‰+ ä»£ç†Cï¼ˆç»¼åˆï¼‰
        æ¯ä¸ªä»£ç†æœ‰æ¸…æ´çš„ä¸Šä¸‹æ–‡ = æ•ˆæœæ›´å¥½
```

### 3. å·¥å…·è®¾è®¡å†³å®šäº†æ•ˆæœ ğŸ”§

```
âŒ å: 15ä¸ªç‰¹å®šå·¥å…·ï¼ˆä»£ç†å›°æƒ‘ï¼‰
âœ… å¥½: 3ä¸ªç»¼åˆå·¥å…·ï¼ˆä»£ç†æ¸…æ™°ï¼‰

åŸåˆ™ï¼šå¦‚æœä½ è‡ªå·±ä¸çŸ¥é“ç”¨å“ªä¸ªå·¥å…·ï¼Œä»£ç†ä¹Ÿä¸çŸ¥é“
```

---

## ğŸ¯ äº”æ­¥æ„å»ºä½ çš„ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“

### ç¬¬ä¸€æ­¥ï¼šä¸‹è½½ç¤ºä¾‹ä»£ç ï¼ˆ1åˆ†é’Ÿï¼‰

ä½ ç°åœ¨æœ‰ 3 ä¸ªæ–‡ä»¶ï¼š
```
ğŸ“„ å¦‚ä½•ç‹¬ç«‹ä½¿ç”¨Agent-Skillsæ„å»ºæ™ºèƒ½ä½“é¡¹ç›®.md
   â””â”€ å®Œæ•´æŒ‡å—ï¼ˆä¾›å‚è€ƒï¼‰

ğŸ“„ LLM-APIé›†æˆæŒ‡å—.md
   â””â”€ é›†æˆä¸åŒ LLM çš„æ–¹æ³•

âœ¨ ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿç¤ºä¾‹.py
   â””â”€ å®Œæ•´å¯è¿è¡Œçš„ç¤ºä¾‹ï¼ˆç«‹å³å¯ç”¨ï¼‰
```

### ç¬¬äºŒæ­¥ï¼šç†è§£åŸºæœ¬æ¶æ„ï¼ˆ2åˆ†é’Ÿï¼‰

æ‰“å¼€ `ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿç¤ºä¾‹.py` å¹¶é˜…è¯»è¿™äº›å…³é”®éƒ¨åˆ†ï¼š

```python
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®ç»“æ„ï¼ˆTask, Message, ResearchResultï¼‰
# ç†è§£è¿™äº›ä»£è¡¨ä»€ä¹ˆ

# ç¬¬äºŒéƒ¨åˆ†ï¼šLLMInterface
# è¿™æ˜¯ä½ æ’å…¥è‡ªå·± LLM API çš„åœ°æ–¹

# ç¬¬ä¸‰éƒ¨åˆ†ï¼šAgent ç±»
# SearchAgentï¼ˆæœç´¢ï¼‰ã€AnalysisAgentï¼ˆåˆ†æï¼‰ã€SynthesisAgentï¼ˆç»¼åˆï¼‰

# ç¬¬å››éƒ¨åˆ†ï¼šResearchCoordinator
# è¿™æ˜¯ç›‘ç£è€… - åè°ƒå…¶ä»–ä»£ç†
```

### ç¬¬ä¸‰æ­¥ï¼šé›†æˆä½ çš„ LLMï¼ˆ1åˆ†é’Ÿï¼‰

ç¼–è¾‘ `ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿç¤ºä¾‹.py`ï¼Œæ‰¾åˆ° `LLMInterface.call()` æ–¹æ³•ï¼š

**å¦‚æœä½¿ç”¨ OpenAIï¼š**
```python
def call(self, prompt, system_prompt=None, temperature=0.7, max_tokens=2000):
    from openai import OpenAI
    client = OpenAI(api_key=self.api_key)
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content
```

**å¦‚æœä½¿ç”¨æœ¬åœ° Ollamaï¼ˆæ¨èå¼€å‘ï¼‰ï¼š**
```python
def call(self, prompt, system_prompt=None, temperature=0.7, max_tokens=2000):
    import requests
    response = requests.post(
        "http://localhost:11434/api/chat",
        json={
            "model": "mistral",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
        }
    )
    return response.json()["message"]["content"]
```

æ›´å¤šé€‰é¡¹è§ï¼š`LLM-APIé›†æˆæŒ‡å—.md`

### ç¬¬å››æ­¥ï¼šè¿è¡Œç¤ºä¾‹ï¼ˆ1åˆ†é’Ÿï¼‰

```bash
# åˆ›å»º .env æ–‡ä»¶ï¼ˆå¦‚æœä½¿ç”¨è¿œç¨‹ APIï¼‰
echo "OPENAI_API_KEY=sk-xxxxx" > .env

# æˆ–å¯åŠ¨æœ¬åœ° Ollama
ollama serve &
ollama pull mistral

# è¿è¡Œ
python ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿç¤ºä¾‹.py
```

### ç¬¬äº”æ­¥ï¼šè‡ªå®šä¹‰ä½ çš„ç³»ç»Ÿï¼ˆç»§ç»­...ï¼‰

ç°åœ¨ä½ æœ‰äº†ä¸€ä¸ªå·¥ä½œçš„ç³»ç»Ÿï¼Œå¯ä»¥è‡ªå®šä¹‰ï¼š

```python
# ä¿®æ”¹ç ”ç©¶è¯é¢˜
research_topics = [
    "ä½ çš„ç¬¬ä¸€ä¸ªè¯é¢˜",
    "ä½ çš„ç¬¬äºŒä¸ªè¯é¢˜"
]

# æ·»åŠ æ›´å¤šä»£ç†
class CustomAgent(Agent):
    def analyze_custom(self, data):
        prompt = f"å¯¹è¿™ä¸ªæ•°æ®åšè‡ªå®šä¹‰åˆ†æ: {data}"
        return self.llm.call(prompt)

# ä¿®æ”¹ä»»åŠ¡åˆ†è§£é€»è¾‘
coordinator.decompose_task()  # æ·»åŠ ä½ çš„å­ä»»åŠ¡

# ä¿å­˜å’ŒæŸ¥è¯¢ç»“æœ
coordinator.save_results(result)
```

---

## ğŸ“Š ç³»ç»Ÿå·¥ä½œæµç¨‹

```
ç”¨æˆ·è¾“å…¥
   â†“
ResearchCoordinatorï¼ˆç›‘ç£è€…ï¼‰
   â”œâ”€ åˆ†è§£ä¸º 3 ä¸ªä»»åŠ¡
   â”‚
   â”œâ”€ ä»»åŠ¡1: æœç´¢ â†’ SearchAgent â†’ LLM è°ƒç”¨ â†’ æœç´¢ç»“æœ
   â”‚
   â”œâ”€ ä»»åŠ¡2: åˆ†æ â†’ AnalysisAgent â†’ LLM è°ƒç”¨ â†’ åˆ†æç»“æœ
   â”‚
   â”œâ”€ ä»»åŠ¡3: ç»¼åˆ â†’ SynthesisAgent â†’ LLM è°ƒç”¨ â†’ æœ€ç»ˆæŠ¥å‘Š
   â”‚
   â””â”€ èšåˆç»“æœ â†’ ä¿å­˜åˆ° JSON
   
æœ€ç»ˆè¾“å‡ºï¼šresearch_output/research_20250115_120000.json
```

---

## ğŸ”§ å¸¸è§è‡ªå®šä¹‰

### æ·»åŠ æ–°çš„ä»£ç†ç±»å‹

```python
class ReviewAgent(Agent):
    """è¯„å®¡ä»£ç† - è´¨é‡æ£€æŸ¥"""
    
    def __init__(self, name, llm):
        super().__init__(name, llm, "reviewer")
    
    def review(self, content: str) -> Dict:
        """è¯„å®¡å†…å®¹è´¨é‡"""
        prompt = f"""è¯„å®¡ä»¥ä¸‹å†…å®¹çš„è´¨é‡ï¼ˆæ‰“åˆ† 1-10ï¼‰:
        
{content}

æ ¼å¼åŒ–è¿”å›:
## è´¨é‡è¯„åˆ†
[1-10]

## åé¦ˆ
[è¯¦ç»†åé¦ˆ]

## æ”¹è¿›å»ºè®®
[å…·ä½“å»ºè®®]"""
        
        response = self.llm.call(prompt)
        return {"content": response, "status": "completed"}
```

### ä¿®æ”¹ä»»åŠ¡åˆ†è§£

```python
def decompose_task(self, topic: str) -> List[Task]:
    """è‡ªå®šä¹‰ä»»åŠ¡åˆ†è§£"""
    
    # ä½ çš„é€»è¾‘
    if "æ¯”è¾ƒ" in topic:
        return [
            Task(..., task_type="search_option_a"),
            Task(..., task_type="search_option_b"),
            Task(..., task_type="compare"),
        ]
    else:
        # é»˜è®¤é€»è¾‘
        return super().decompose_task(topic)
```

### æ·»åŠ å†…å­˜æŒä¹…åŒ–

```python
def save_to_database(self, result: ResearchResult):
    """ä¿å­˜åˆ°æ•°æ®åº“è€Œä¸ä»…ä»…æ˜¯ JSON"""
    
    # ä½¿ç”¨ä½ é€‰æ‹©çš„æ•°æ®åº“ï¼š
    # - SQLiteï¼ˆç®€å•é¡¹ç›®ï¼‰
    # - PostgreSQLï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
    # - MongoDBï¼ˆçµæ´»æ•°æ®ï¼‰
    
    import sqlite3
    conn = sqlite3.connect("research.db")
    cursor = conn.cursor()
    
    cursor.execute("""
        INSERT INTO research (topic, content, timestamp)
        VALUES (?, ?, ?)
    """, (result.topic, json.dumps(asdict(result)), datetime.now()))
    
    conn.commit()
```

### æ·»åŠ æˆæœ¬è¿½è¸ª

```python
class CostAwareLLM(LLMInterface):
    """è¿½è¸ª API æˆæœ¬"""
    
    def __init__(self, api_key, model, pricing=None):
        super().__init__(api_key, model)
        self.pricing = pricing or {
            "gpt-4o": {"input": 0.003, "output": 0.012},
        }
        self.total_cost = 0.0
    
    def call(self, prompt, system_prompt=None, ...):
        response = super().call(prompt, system_prompt, ...)
        
        # è®¡ç®—æˆæœ¬
        input_tokens = len(prompt.split()) * 1.3  # ç²—ç•¥ä¼°è®¡
        output_tokens = len(response.split()) * 1.3
        
        if self.model in self.pricing:
            pricing = self.pricing[self.model]
            cost = (input_tokens / 1000 * pricing["input"] + 
                   output_tokens / 1000 * pricing["output"])
            self.total_cost += cost
        
        return response
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. å¹¶è¡Œæ‰§è¡Œä»£ç†

```python
from concurrent.futures import ThreadPoolExecutor

def execute_tasks_parallel(self, tasks):
    """å¹¶è¡Œæ‰§è¡Œä»»åŠ¡è€Œä¸æ˜¯é¡ºåºæ‰§è¡Œ"""
    
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {executor.submit(self.execute_task, task): task 
                  for task in tasks}
        
        results = []
        for future in futures:
            results.append(future.result())
    
    return results
```

### 2. ç¼“å­˜ LLM å“åº”

```python
from functools import lru_cache

class CachingLLM(LLMInterface):
    @lru_cache(maxsize=100)
    def call(self, prompt, system_prompt=None):
        """ç¼“å­˜ç›¸åŒè¾“å…¥çš„ LLM å“åº”"""
        return super().call(prompt, system_prompt)
```

### 3. åˆ†æ‰¹å¤„ç†

```python
def batch_research(self, topics: List[str]) -> List[ResearchResult]:
    """ä¸€æ¬¡å¤„ç†å¤šä¸ªè¯é¢˜ï¼Œå…±äº«èµ„æº"""
    
    results = []
    for topic in topics:
        result = self.research(topic)
        results.append(result)
    
    # æœ€åç»Ÿä¸€ä¿å­˜
    for result in results:
        self.save_results(result)
    
    return results
```

---

## ğŸ› è°ƒè¯•æŠ€å·§

### å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# åœ¨ä½ çš„ä»£ç ä¸­
logger.debug(f"å‘é€æç¤ºç»™ {agent.name}: {prompt[:100]}...")
logger.info(f"æ”¶åˆ°ç»“æœ: {len(response)} å­—ç¬¦")
logger.warning(f"API è°ƒç”¨ #{llm.call_count}")
logger.error(f"å¤±è´¥: {error}")
```

### ä¿å­˜ä¸­é—´ç»“æœ

```python
def debug_save_intermediate(self, task, result):
    """ä¿å­˜æ¯ä¸€æ­¥çš„ç»“æœç”¨äºè°ƒè¯•"""
    
    debug_dir = Path("debug_output")
    debug_dir.mkdir(exist_ok=True)
    
    debug_file = debug_dir / f"{task.id}_{task.status}.json"
    with open(debug_file, "w") as f:
        json.dump({
            "task": asdict(task),
            "result": result
        }, f, indent=2, ensure_ascii=False)
```

### æµ‹è¯•å•ä¸ªä»£ç†

```python
# ä¸è¿è¡Œæ•´ä¸ªç³»ç»Ÿï¼Œåªæµ‹è¯•ä¸€ä¸ªä»£ç†
llm = LLMInterface(api_key="test")
search_agent = SearchAgent("TestSearcher", llm)

result = search_agent.search("æµ‹è¯•æŸ¥è¯¢")
print(json.dumps(result, indent=2, ensure_ascii=False))
```

---

## ğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ 

| è¯é¢˜ | æ–‡ä»¶ | æ—¶é—´ |
|------|------|------|
| ç†è®ºåŸºç¡€ | `å¦‚ä½•ç‹¬ç«‹ä½¿ç”¨Agent-Skills...md` | 1å°æ—¶ |
| LLM é›†æˆ | `LLM-APIé›†æˆæŒ‡å—.md` | 30åˆ†é’Ÿ |
| é«˜çº§æ¨¡å¼ | åŸå§‹é¡¹ç›®ä¸­çš„ `skills/` | 2-3å°æ—¶ |
| å®é™…æ„å»º | ä¿®æ”¹ç¤ºä¾‹ä»£ç  | è¿›è¡Œä¸­... |

---

## ğŸ’¡ å®ç”¨å»ºè®®

1. **ä»ç®€å•å¼€å§‹** - å•ä»£ç† â†’ å¤šä»£ç†
2. **æœ¬åœ°æµ‹è¯•** - ä½¿ç”¨ Ollama å¼€å‘ï¼ŒOpenAI ç”Ÿäº§
3. **è·Ÿè¸ªæˆæœ¬** - ä»ç¬¬ä¸€ä¸ª API è°ƒç”¨å°±å¼€å§‹
4. **ä¿å­˜ä¸€åˆ‡** - æ‰€æœ‰ç»“æœä¿å­˜ä¸º JSONï¼Œä¾¿äºåˆ†æ
5. **å®šæœŸè¯„ä¼°** - å®šæœŸæ£€æŸ¥ç³»ç»Ÿè´¨é‡æŒ‡æ ‡

---

## ğŸ“ æ¶æ„æ¨¡å¼é€ŸæŸ¥è¡¨

### é€‚ç”¨äºç®€å•ä»»åŠ¡
```
è¾“å…¥ â†’ å•ä»£ç†(æœç´¢+åˆ†æ+ç»¼åˆ) â†’ è¾“å‡º
æˆæœ¬: ä½ | é€Ÿåº¦: å¿« | è´¨é‡: ä¸­ç­‰
```

### é€‚ç”¨äºå¤æ‚ä»»åŠ¡ï¼ˆæ¨èï¼‰
```
è¾“å…¥ â†’ ç›‘ç£è€… â†’ æœç´¢ä»£ç† + åˆ†æä»£ç† + ç»¼åˆä»£ç† â†’ è¾“å‡º
æˆæœ¬: ä¸­ç­‰ | é€Ÿåº¦: ä¸­ç­‰ | è´¨é‡: é«˜
```

### é€‚ç”¨äºå®æ—¶åº”ç”¨
```
è¾“å…¥ â†’ ç¼“å­˜æŸ¥è¯¢ â†’ å¹¶è¡Œä»£ç†ç¾¤ â†’ å¿«é€Ÿè¾“å‡º
æˆæœ¬: ä½ | é€Ÿåº¦: âš¡âš¡âš¡ | è´¨é‡: ä¸­ç­‰-é«˜
```

---

## â“ å¸¸è§é—®é¢˜

**Q: æˆ‘éœ€è¦æ”¹å˜ä»€ä¹ˆæ‰èƒ½ç”¨æˆ‘è‡ªå·±çš„ LLMï¼Ÿ**
A: åªéœ€æ”¹å˜ `LLMInterface.call()` æ–¹æ³•ã€‚å‚è€ƒ `LLM-APIé›†æˆæŒ‡å—.md`

**Q: æˆ‘å¯ä»¥æ·»åŠ æ›´å¤šä»£ç†å—ï¼Ÿ**
A: å½“ç„¶ï¼ç»§æ‰¿ `Agent` ç±»å¹¶åœ¨ `ResearchCoordinator` ä¸­æ³¨å†Œ

**Q: å¦‚ä½•é™ä½æˆæœ¬ï¼Ÿ**
A: ä½¿ç”¨æœ¬åœ° Ollamaï¼ˆå…è´¹ï¼‰æˆ– Groqï¼ˆä¾¿å®œï¼‰

**Q: æˆ‘å¯ä»¥ç¦»çº¿ä½¿ç”¨å—ï¼Ÿ**
A: æ˜¯çš„ï¼Œä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹

**Q: è¿™å¦‚ä½•ä¸ Claude/Cursor ç›¸æ¯”ï¼Ÿ**
A: å®Œå…¨ç‹¬ç«‹ã€‚è¿™äº› Skills ä¸ä»»ä½•å¹³å°æ— å…³ï¼Œè¿™åªæ˜¯ä¸€ä¸ªå®ç°ç¤ºä¾‹

---

## ğŸš€ ç°åœ¨å°±å¼€å§‹

```bash
# 1. å‡†å¤‡ç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. å¯åŠ¨æœ¬åœ° LLMï¼ˆå¯é€‰ï¼‰
ollama serve &
ollama pull mistral

# 3. è¿è¡Œç¤ºä¾‹
python ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿç¤ºä¾‹.py

# 4. è‡ªå®šä¹‰ä»£ç 
# ç¼–è¾‘ ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿç¤ºä¾‹.py ä¿®æ”¹ LLMInterface æˆ–æ·»åŠ æ–°ä»£ç†

# 5. æŸ¥çœ‹ç»“æœ
cat research_output/research_*.json
```

ç¥ä½ æ„å»ºæˆåŠŸï¼å¦‚æœ‰é—®é¢˜ï¼Œå‚è€ƒå®Œæ•´æŒ‡å—æˆ–åŸå§‹ Agent Skills é¡¹ç›®ã€‚

---

**è®°ä½ï¼š** 
- ğŸ¯ ä¸Šä¸‹æ–‡æ˜¯æœ‰é™èµ„æºï¼Œç²¾å¿ƒé€‰æ‹©
- ğŸ¤ ç”¨å¤šä»£ç†æ¶æ„éš”ç¦»é—®é¢˜
- ğŸ”§ è®¾è®¡å¥½å·¥å…·ï¼Œä»£ç†å°±èƒ½ç”¨å¥½
- ğŸ“Š ä»ç¬¬ä¸€å¤©å°±è¿½è¸ªæŒ‡æ ‡
- ğŸš€ ä»ç®€å•åˆ°å¤æ‚ï¼Œè¿­ä»£æ”¹è¿›

