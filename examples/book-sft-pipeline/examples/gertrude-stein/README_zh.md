# 示例：格特鲁德·斯坦因风格迁移

一个完整的案例研究，使用她 1909 年的小说《三个女人》训练 Qwen3-8B-Base 以格特鲁德·斯坦因的风格写作。

## 项目总结

| 指标 | 数值 |
|--------|-------|
| 源书 | 三个女人（1909） |
| 书籍字数 | 约 86,000 |
| 训练示例 | 592 |
| 测试示例 | 50 |
| 模型 | Qwen/Qwen3-8B-Base |
| LoRA 秩 | 32 |
| 适配器大小 | 352 MB |
| 训练时间 | 约 15 分钟 |
| 总成本 | 约 $2 |

## 训练指标

### 损失轨迹

| 步骤 | 训练损失 | 测试损失 |
|------|-----------|-----------|
| 5 | 7579.67 | - |
| 40 | 4032.35 | 7584.85 |
| 100 | 2392.17 | 2127.44 |
| 140 | 2863.17 | 752.95 |
| 180 | 1327.41 | 229.88 |
| 200 | 1044.49 | 213.36 |
| 最终 | 约 800 | 约 200 |

损失降低：97%（测试损失从 7584 到 213）

## 学到的风格标记

模型成功学到了这些格特鲁德·斯坦因的特征：

1. **重复的句子结构**："她是个好女人。她总是个好女人。"
2. **简单词汇**：常用词，没有华丽的语言
3. **逗号分隔的形容词**："一个黑色的、甜蜜的、小的、漂亮的女孩"
4. **现在进行时**："她总是在做"、"他在思考"
5. **人物专注**：深入的心理观察而不是情节
6. **有节奏、迷幻的质量**：几乎是音乐般的散文

## 示例输出

### 现代场景：房地产办公室

**提示**：以格特鲁德·斯坦因的风格写关于一个房地产职员下班回家疲劳的故事。

**输出**：
> 这是房地产办公室职员很忙碌的一天。他来到他在镇子工薪阶层部分的小房子，非常疲劳。他走进厨房，他的妻子正在做饭。
>
> "嗨，姐姐，回来吃晚饭了吗？"他对她说。
>
> 她看着他，看到他非常疲劳。她看着他，然后转向看向火。她没有回答他。
>
> 他脱掉了帽子和外套，然后坐在门内的台阶上休息。他今天非常疲劳，他坐在那里，只是有点打瞌睡。

**分析**：验证的原始内容 - "房地产办公室"、"工薪阶层"、"嗨，姐姐" 不出现在训练数据中。

### 现代场景：短信

**提示**：写关于两个情侣通过短信交流的故事。

**输出**：
> "嗨，赫尔曼，我刚刚有一分钟离开了我的病人。想和你聊一会儿吗，赫尔曼，"莱娜一个晚上写道，然后她给他发了短信。
>
> 赫尔曼读了它，然后他给她写回去，"我正准备睡觉，莱娜。明天想来见我吗，莱娜。想给我带点东西吗，莱娜。想给我买冰淇淋吗，莱娜。想给我买一些那些小甜甜圈吗，莱娜......"

**分析**：经典的斯坦因排比（句子开头的重复）应用于现代短信格式。

## AI 检测器结果

使用 Pangram AI 检测器测试：**100% 人类写作**

测试了多个样本，全部被评分为人类写作散文。

## 验证方法

### 现代场景测试

我们在 1909 年不可能存在的场景上测试了模型：
- 咖啡师制作拿铁
- 社交媒体滚动
- 视频通话
- 食品配送司机
- 气候变化焦虑

当风格标记出现在现代背景中时，它证明了模型学到了**风格**而不是**内容**。

### 原创性验证

在训练数据中搜索了输出短语：

```bash
grep "房地产办公室" dataset.jsonl    # 无匹配
grep "工薪阶层" dataset.jsonl          # 无匹配
grep "嗨，姐姐" dataset.jsonl          # 无匹配
grep "短信" dataset.jsonl          # 无匹配
```

## 已知限制

### 角色名称泄露（约 30% 的输出）

模型有时在现代场景中使用原始角色名称（Melanctha、Mrs. Lehntman、Anna）。这是因为来自一本书的 592 个例子意味着这些名称出现了数百次。

**缓解**：训练同一作者的多本书，或添加具有不同名称的合成示例。

### 成功率分布

- 完美风格迁移：约 50%
- 带名称泄露的风格：约 30%
- 部分风格：约 15%
- 失败：约 5%

50% 的完美率对于在一本书上训练的 8B 模型是现实的。

## 使用的配置

### 数据集生成

```python
CONFIG = {
    "min_words": 150,
    "max_words": 400,
    "overlap": True,  # 最后一个段落进入下一个块
    "variants_per_chunk": 2,
    "prompt_templates": 15,
    "system_prompts": 5,
    "instruction_model": "gemini-2.0-flash-lite",
}
```

### 训练

```python
CONFIG = {
    "model_name": "Qwen/Qwen3-8B-Base",
    "lora_rank": 32,
    "learning_rate": 5e-4,
    "batch_size": 4,
    "epochs": 3,
    "eval_every": 20,
    "save_every": 50,
}
```

## 关键学习

1. **更小的块效果更好**：150-400 字产生了比 250-650 更多的示例和更好的风格迁移

2. **提示多样性至关重要**：15 个模板 × 5 个系统提示 = 75 种变化防止了记忆化

3. **基础模型优于指令调优**：Qwen3-8B-Base 比指令调优版本更具可塑性

4. **现代场景测试证明了迁移**：如果风格应用于现代背景，模型学到了模式，而不是内容

5. **约 $2 就足够了**：指令生成的 LLM 调用（约 $0.50）加上 Tinker 训练（约 $1.50）

## 文件

- `sample_outputs.md` - 完整的模型输出和分析
- `training_config.json` - 使用的确切配置
- `dataset_sample.jsonl` - 样本训练示例

